# stonet - Testing Strategy

## Overview

This document defines the comprehensive testing strategy for the stonet platform, including testing types, tools, processes, and quality standards.

## Testing Philosophy

**Quality Goals:**
- Zero critical bugs in production
- 80%+ code coverage across all components
- Sub-second API response times
- Accessible to WCAG 2.1 AA standards
- Cross-browser compatibility
- Mobile responsive on all devices

**Testing Principles:**
- Test early, test often
- Automate where possible
- Test in production-like environments
- Security testing throughout development
- Performance testing is not optional
- Accessibility testing is mandatory

## Testing Types and Coverage

### 1. Unit Testing

**Purpose:** Test individual functions, components, and modules in isolation

**Coverage Target:** 80% minimum

**Frontend Unit Testing:**
- **Tool:** Vitest + React Testing Library
- **What to Test:**
  - React components (rendering, props, state)
  - Custom hooks
  - Utility functions
  - Form validation logic
  - State management
  - API service functions

**Example Test Structure:**
```typescript
// Component test example
describe('PostCard', () => {
  it('renders post content correctly', () => {
    render(<PostCard post={mockPost} />);
    expect(screen.getByText(mockPost.content)).toBeInTheDocument();
  });

  it('calls onLike when like button clicked', () => {
    const onLike = vi.fn();
    render(<PostCard post={mockPost} onLike={onLike} />);
    fireEvent.click(screen.getByRole('button', { name: /like/i }));
    expect(onLike).toHaveBeenCalledWith(mockPost.id);
  });
});
```

**Backend Unit Testing:**
- **Tool:** Jest / Vitest
- **What to Test:**
  - Database queries
  - Business logic functions
  - Validation functions
  - Utility functions
  - Data transformations

**Running Tests:**
```bash
# Frontend
cd frontend
npm run test              # Run all tests
npm run test:watch        # Watch mode
npm run test:coverage     # With coverage report

# Backend
cd supabase/functions
npm run test
```

**Acceptance Criteria:**
- All tests pass
- No skipped tests
- Coverage >= 80%
- Tests run in <30 seconds

---

### 2. Integration Testing

**Purpose:** Test how multiple components/modules work together

**Coverage Target:** All critical user flows

**Frontend Integration Testing:**
- **Tool:** Vitest + React Testing Library
- **What to Test:**
  - Form submission flows
  - API integration
  - State management integration
  - Component interactions
  - Routing and navigation

**Backend Integration Testing:**
- **Tool:** Supertest + Jest
- **What to Test:**
  - API endpoints
  - Database interactions
  - Authentication flows
  - File upload workflows
  - Real-time features

**Example:**
```typescript
describe('User Authentication Flow', () => {
  it('allows user to sign up and log in', async () => {
    // Sign up
    const signupRes = await request(app)
      .post('/auth/signup')
      .send({ email: 'test@example.com', password: 'password123' });
    expect(signupRes.status).toBe(201);

    // Log in
    const loginRes = await request(app)
      .post('/auth/login')
      .send({ email: 'test@example.com', password: 'password123' });
    expect(loginRes.status).toBe(200);
    expect(loginRes.body).toHaveProperty('access_token');
  });
});
```

**Acceptance Criteria:**
- All critical flows tested
- Tests use test database
- Tests are independent (can run in any order)
- Cleanup after tests

---

### 3. End-to-End (E2E) Testing

**Purpose:** Test complete user journeys in a real browser

**Coverage Target:** All critical user paths

**Tool:** Playwright

**What to Test:**
- Complete user workflows
- Cross-browser compatibility
- Multi-device testing
- User interactions
- Visual regression

**Critical E2E Test Scenarios:**

1. **User Registration and Onboarding**
   - Sign up with email
   - Complete onboarding flow
   - Verify profile creation

2. **Authentication**
   - Log in with email
   - Log in with OAuth (Google, GitHub, LinkedIn)
   - Password reset flow
   - Session persistence

3. **Post Creation and Interaction**
   - Create text post
   - Create post with image
   - Create post with poll
   - Like, comment, repost
   - Bookmark post

4. **Social Feed**
   - View "For You" feed
   - View "Following" feed
   - Infinite scroll
   - Feed refresh

5. **Connections**
   - Send connection request
   - Accept connection request
   - View connections
   - Search for users

6. **Messaging**
   - Start conversation
   - Send messages
   - Receive messages in real-time
   - Mark as read

7. **Profile Management**
   - Edit profile
   - Add work experience
   - Add education
   - Upload avatar

**Example E2E Test:**
```typescript
test('user can create and interact with a post', async ({ page }) => {
  // Login
  await page.goto('/login');
  await page.fill('[name="email"]', 'test@example.com');
  await page.fill('[name="password"]', 'password123');
  await page.click('button[type="submit"]');
  
  // Create post
  await page.click('button:has-text("Create Post")');
  await page.fill('textarea[placeholder="What\'s on your mind?"]', 'Test post content');
  await page.click('button:has-text("Post")');
  
  // Verify post appears
  await expect(page.locator('text=Test post content')).toBeVisible();
  
  // Like post
  await page.click('button[aria-label="Like"]');
  await expect(page.locator('text=1 like')).toBeVisible();
});
```

**Running E2E Tests:**
```bash
cd frontend
npm run test:e2e              # Run all E2E tests
npm run test:e2e:ui           # Open Playwright UI
npm run test:e2e:chromium     # Test in Chromium only
npm run test:e2e:firefox      # Test in Firefox
npm run test:e2e:webkit       # Test in WebKit (Safari)
```

**Browsers to Test:**
- Chromium (Chrome, Edge)
- Firefox
- WebKit (Safari)

**Devices to Test:**
- Desktop (1920x1080, 1366x768)
- Tablet (768x1024)
- Mobile (375x667, 414x896)

**Acceptance Criteria:**
- All critical paths pass
- Cross-browser compatible
- Responsive on all devices
- No console errors
- Visual consistency

---

### 4. API Testing

**Purpose:** Verify API contracts, responses, and error handling

**Tool:** Postman / Newman (CLI)

**What to Test:**
- Request/response formats
- Status codes
- Authentication and authorization
- Input validation
- Error handling
- Rate limiting
- Pagination

**Test Collections:**
1. **Authentication APIs**
   - Sign up, login, logout, refresh
   - OAuth flows
   - Phone authentication

2. **User APIs**
   - Profile CRUD
   - Work experience CRUD
   - Education CRUD
   - Skills management

3. **Post APIs**
   - Feed generation
   - Post creation
   - Post interactions
   - Comments

4. **Connection APIs**
   - Send/accept requests
   - Connection list
   - Suggestions

5. **Message APIs**
   - Send message
   - Get messages
   - Conversation management

**Running API Tests:**
```bash
newman run api-tests.postman_collection.json \
  --environment dev.postman_environment.json \
  --reporters cli,html
```

**Acceptance Criteria:**
- All endpoints return correct status codes
- Response schemas match documentation
- Error responses are consistent
- Authentication required where specified
- Rate limiting works

---

### 5. Performance Testing

**Purpose:** Ensure application meets performance benchmarks

**Tools:** 
- Lighthouse (frontend)
- k6 or Artillery (load testing)
- Chrome DevTools Performance tab

**Performance Benchmarks:**

**Frontend Performance:**
- First Contentful Paint (FCP): < 1.5s
- Largest Contentful Paint (LCP): < 2.5s
- Time to Interactive (TTI): < 3.5s
- Cumulative Layout Shift (CLS): < 0.1
- First Input Delay (FID): < 100ms
- Lighthouse score: > 90

**API Performance:**
- Authentication: < 500ms
- Feed loading: < 1s
- Post creation: < 500ms
- Profile loading: < 800ms
- Message sending: < 300ms
- Search: < 500ms

**Load Testing Scenarios:**

1. **Normal Load**
   - 100 concurrent users
   - Duration: 10 minutes
   - Expected: No errors, response times within SLA

2. **Peak Load**
   - 1000 concurrent users
   - Duration: 5 minutes
   - Expected: < 1% error rate, response times within acceptable range

3. **Stress Test**
   - Gradually increase to 2000 users
   - Identify breaking point
   - Expected: Graceful degradation

**Running Performance Tests:**
```bash
# Lighthouse
npm run lighthouse

# Load testing with k6
k6 run --vus 100 --duration 10m load-test.js

# Bundle size analysis
npm run build:analyze
```

**Acceptance Criteria:**
- All benchmarks met
- No memory leaks
- Acceptable performance under load
- Bundle size optimized (< 500KB initial)

---

### 6. Security Testing

**Purpose:** Identify and fix security vulnerabilities

**Tools:**
- OWASP ZAP (penetration testing)
- Snyk (dependency scanning)
- npm audit (frontend dependencies)
- ESLint security plugins
- Manual security review

**Security Test Areas:**

1. **Authentication & Authorization**
   - JWT token security
   - Session management
   - OAuth implementation
   - Password security
   - Rate limiting

2. **Input Validation**
   - SQL injection prevention
   - XSS prevention
   - CSRF protection
   - File upload validation

3. **Data Protection**
   - Row Level Security (RLS) policies
   - Data encryption
   - Sensitive data handling
   - Privacy settings enforcement

4. **API Security**
   - CORS configuration
   - API rate limiting
   - Error message security
   - Authentication required

**Security Checklist:**
- [ ] All inputs validated and sanitized
- [ ] RLS policies enabled on all tables
- [ ] Passwords hashed (never stored plain text)
- [ ] JWT tokens properly validated
- [ ] Secrets not in code (use env variables)
- [ ] HTTPS enforced
- [ ] CORS properly configured
- [ ] Rate limiting implemented
- [ ] File uploads validated (type, size)
- [ ] SQL injection prevented (parameterized queries)
- [ ] XSS prevented (escaped outputs)
- [ ] CSRF protection implemented
- [ ] Sensitive data logged properly

**Running Security Tests:**
```bash
# Dependency audit
npm audit
npm audit fix

# Snyk scanning
snyk test
snyk monitor

# OWASP ZAP (manual/automated)
# Run ZAP proxy and scan application
```

**Acceptance Criteria:**
- Zero critical vulnerabilities
- Zero high-priority vulnerabilities
- All medium vulnerabilities documented and accepted/fixed
- Security audit passed

---

### 7. Accessibility Testing

**Purpose:** Ensure application is accessible to all users

**Standards:** WCAG 2.1 Level AA compliance

**Tools:**
- axe DevTools (browser extension)
- Lighthouse accessibility audit
- Screen readers (NVDA, JAWS, VoiceOver)
- Keyboard navigation testing

**Accessibility Checklist:**
- [ ] Proper heading hierarchy (h1, h2, h3)
- [ ] All images have alt text
- [ ] Forms have labels
- [ ] Buttons have descriptive text
- [ ] Color contrast >= 4.5:1 (text), >= 3:1 (large text)
- [ ] Keyboard navigation works
- [ ] Focus indicators visible
- [ ] Screen reader announces content correctly
- [ ] ARIA labels where needed
- [ ] No keyboard traps
- [ ] Skip links present
- [ ] Error messages associated with fields
- [ ] Semantic HTML used

**Testing Process:**
1. Run automated axe scan
2. Lighthouse accessibility audit
3. Keyboard-only navigation test
4. Screen reader test (at least one)
5. Color contrast verification
6. Manual review

**Acceptance Criteria:**
- Zero accessibility violations (critical/serious)
- Lighthouse accessibility score > 95
- Keyboard navigation fully functional
- Screen reader friendly

---

### 8. Database Testing

**Purpose:** Verify database performance, integrity, and security

**What to Test:**
- Migration scripts
- RLS policies
- Triggers and functions
- Indexes effectiveness
- Query performance
- Data integrity constraints

**Test Scenarios:**
1. **Migration Testing**
   - Migrations run without errors
   - Migrations are reversible
   - No data loss during migration

2. **RLS Policy Testing**
   - Users can only access their data
   - Public data accessible
   - Connection-only data restricted
   - Admin access works

3. **Performance Testing**
   - Feed query < 500ms
   - Search query < 500ms
   - Write operations < 200ms
   - Index usage verified

4. **Data Integrity Testing**
   - Foreign key constraints work
   - Unique constraints enforced
   - Check constraints validated
   - Cascade deletes work correctly

**Tools:**
- pgTAP (PostgreSQL testing)
- EXPLAIN ANALYZE (query performance)
- Supabase Studio (RLS testing)

**Acceptance Criteria:**
- All migrations tested
- RLS policies verified
- Performance benchmarks met
- Data integrity ensured

---

## Testing Schedule

### Sprint Testing (Every 2 Weeks)

**Week 1:**
- Day 1-4: Development with unit tests
- Day 5: Integration testing begins

**Week 2:**
- Day 1-3: Feature completion and testing
- Day 4: E2E testing and bug fixes
- Day 5: Sprint review and regression testing

### Phase Testing

**End of Phase 1 (Week 2):**
- Full integration testing
- Security scan
- Performance baseline

**End of Phase 2 (Week 4):**
- E2E critical paths
- Load testing (100 users)
- Accessibility audit

**End of Phase 3 (Week 6):**
- Complete E2E suite
- Load testing (1000 users)
- Security audit

**Phase 4 (Week 7-8):**
- Comprehensive testing
- User acceptance testing
- Performance optimization
- Final security audit

## Test Environments

### 1. Local Development
- **Purpose:** Developer testing
- **Database:** Local Supabase instance or dev database
- **Data:** Mock/test data
- **Access:** Individual developers

### 2. Staging
- **Purpose:** Integration and E2E testing
- **Database:** Staging database (isolated from prod)
- **Data:** Realistic test data
- **Access:** All team members
- **Deployment:** Automatic on merge to develop

### 3. UAT (User Acceptance Testing)
- **Purpose:** Beta testing with real users
- **Database:** UAT database
- **Data:** Production-like data
- **Access:** Beta testers and team
- **Deployment:** Manual trigger

### 4. Production
- **Purpose:** Live application
- **Database:** Production database
- **Data:** Real user data
- **Access:** End users
- **Deployment:** Manual approval required

## Test Data Management

### Test Data Strategy

**Mock Data:**
- Generated programmatically
- Realistic but not real
- Covers edge cases
- Reset between tests

**Seed Data:**
- Consistent test accounts
- Standard test scenarios
- Available in all environments (except prod)

**Test Users:**
```
Admin User:
Email: admin@stonet.test
Password: TestAdmin123!

Regular User:
Email: user@stonet.test
Password: TestUser123!

Test User with Connections:
Email: connected@stonet.test
Password: TestConnected123!
```

### Data Reset Procedures

**After Each Test:**
- Cleanup created data
- Restore original state
- Clear cache/storage

**Daily:**
- Reset staging database
- Reload seed data
- Clear test uploads

## Bug Tracking and Management

### Bug Severity Levels

**Critical (P0):**
- System down or unusable
- Data loss
- Security breach
- Fix immediately

**High (P1):**
- Major feature broken
- Significant user impact
- Workaround difficult
- Fix within 24 hours

**Medium (P2):**
- Feature partially works
- Moderate user impact
- Workaround available
- Fix within 1 week

**Low (P3):**
- Minor issue
- Minimal user impact
- Cosmetic issues
- Fix when possible

### Bug Report Template

```markdown
**Title:** [Brief description]

**Severity:** [Critical/High/Medium/Low]

**Environment:** [Local/Staging/Production]

**Steps to Reproduce:**
1. Step 1
2. Step 2
3. Step 3

**Expected Behavior:**
[What should happen]

**Actual Behavior:**
[What actually happens]

**Screenshots/Videos:**
[Attach if applicable]

**Browser/Device:**
- Browser: Chrome 120
- OS: Windows 11
- Device: Desktop

**Console Errors:**
[Paste any console errors]

**Additional Context:**
[Any other relevant information]
```

### Bug Workflow

1. **Reported:** Bug discovered and logged
2. **Triaged:** Severity assigned, priority set
3. **Assigned:** Developer assigned
4. **In Progress:** Being fixed
5. **Fixed:** Code committed
6. **In Testing:** QA verifying fix
7. **Verified:** Fix confirmed
8. **Closed:** Bug resolved

## Testing Tools Summary

| Testing Type | Tool | Owner |
|--------------|------|-------|
| Frontend Unit | Vitest + RTL | Frontend Team |
| Backend Unit | Jest/Vitest | Backend Team |
| Integration | Supertest | Backend Team |
| E2E | Playwright | QA Team |
| API | Postman/Newman | QA Team |
| Performance | Lighthouse, k6 | QA + DevOps |
| Security | OWASP ZAP, Snyk | Security/DevOps |
| Accessibility | axe, Lighthouse | Frontend + QA |
| Database | pgTAP | Database Team |

## Continuous Integration

### CI Pipeline Checks

**On Every Pull Request:**
1. Lint check
2. Type check (TypeScript)
3. Unit tests
4. Integration tests
5. Build verification
6. Security scan (dependencies)
7. Code coverage report

**On Merge to Develop:**
1. All PR checks
2. E2E tests (critical paths)
3. Deploy to staging
4. Smoke tests in staging

**On Merge to Main:**
1. All checks from develop
2. Full E2E suite
3. Performance testing
4. Security scan
5. Deploy to production
6. Smoke tests in production

### CI Configuration

```yaml
# .github/workflows/test.yml
name: Test Suite

on: [pull_request]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - uses: actions/setup-node@v3
      - run: npm install
      - run: npm run lint
      - run: npm run type-check
      - run: npm run test:coverage
      - run: npm run build
      - uses: codecov/codecov-action@v3
```

## Quality Gates

### Definition of Done (DoD)

A feature is complete when:
- [ ] Code written and reviewed
- [ ] Unit tests written (80% coverage)
- [ ] Integration tests passed
- [ ] E2E tests passed (if applicable)
- [ ] No critical or high bugs
- [ ] Performance benchmarks met
- [ ] Accessibility checked
- [ ] Documentation updated
- [ ] QA signed off
- [ ] Deployed to staging and verified

### Release Criteria

Production release requires:
- [ ] All DoD items complete for all features
- [ ] Full E2E suite passed
- [ ] Security audit passed
- [ ] Performance testing passed
- [ ] Load testing passed (1000 users)
- [ ] Zero critical bugs
- [ ] < 5 high-priority bugs
- [ ] Accessibility audit passed
- [ ] UAT feedback addressed
- [ ] Rollback plan documented
- [ ] Monitoring configured
- [ ] Stakeholder approval

## Document History

| Version | Date | Author | Changes |
|---------|------|--------|---------|
| 1.0 | Jan 20, 2026 | TBD | Initial testing strategy |
